{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb3f894-11fe-4585-98b1-2fa382780f9a",
   "metadata": {},
   "source": [
    "# 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab643d22-7982-4515-99db-e1d2105855a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import struct\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "target_size = (128, 128)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96639ce-bba5-44a6-ae17-a447ab25f194",
   "metadata": {},
   "source": [
    "# 함수 및 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b928e8d-1bab-49ad-90b9-1477d336f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_dat_file(file_name, data):\n",
    "    data = np.array(data)\n",
    "    data = np.mean(np.array(data), axis=2)\n",
    "    height, width = data.shape\n",
    "\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    with open(file_name, 'wb') as fout:\n",
    "        fout.write(width.to_bytes(4, byteorder='little'))\n",
    "        fout.write(height.to_bytes(4, byteorder='little'))\n",
    "\n",
    "        for row in data:\n",
    "            for pixel in row:\n",
    "                fout.write(struct.pack('f', pixel.item()))   \n",
    "\n",
    "def load_to_file(file_name):\n",
    "    with open(file_name, 'rb') as fin:\n",
    "\n",
    "        width = int.from_bytes(fin.read(4), byteorder='little')\n",
    "        height = int.from_bytes(fin.read(4), byteorder='little')\n",
    "\n",
    "        data_size = width * height\n",
    "        \n",
    "        pData = [struct.unpack('f', fin.read(4)) for _ in range(data_size)]\n",
    "        pData = np.array(pData).reshape((height,width,1))\n",
    "\n",
    "        pData = pData.astype(np.float32)\n",
    "\n",
    "        return pData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0b42b3-c13b-4c80-93e8-8c5396258033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_file(filename):\n",
    "    image = Image.open(filename).convert('RGB') \n",
    "    image_array = np.array(image)    \n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93039acc-2cf8-49a9-b27d-91d52a040046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pred, target) :\n",
    "    loss_mse = F.mse_loss(pred, target)\n",
    "    loss_mae = F.l1_loss(pred, target)\n",
    "    loss = 0.5 * loss_mae + 0.5 * loss_mse\n",
    "    return loss\n",
    "\n",
    "def custom_loss(pred, target):\n",
    "    \"\"\"\n",
    "    pred: [B,1,H,W] 또는 [B,2,H,W] (채널0=depth, 채널1=logvar)\n",
    "    target: [B,1,H,W]\n",
    "    - target 내 NaN/Inf 는 자동 무시합니다(있다면).\n",
    "    - 불확실성 헤드가 있을 경우(2채널) Gaussian NLL + 약한 엣지 정렬.\n",
    "    - 없을 경우(1채널) L1 + 엣지 정렬.\n",
    "    \"\"\"\n",
    "    assert pred.dim()==4 and target.dim()==4, \"pred/target must be [B,C,H,W]\"\n",
    "    B = pred.size(0)\n",
    "\n",
    "    # 유효 픽셀 마스크: target이 finite인 곳만 사용\n",
    "    valid = torch.isfinite(target).float()\n",
    "    target = torch.nan_to_num(target, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    if pred.size(1) == 2:\n",
    "        mu     = pred[:, 0:1]\n",
    "        logvar = pred[:, 1:2].clamp(-10.0, 10.0)  # 수렴 안정화\n",
    "        diff2  = (mu - target) ** 2\n",
    "        nll    = (diff2 * torch.exp(-logvar) + logvar) * valid\n",
    "\n",
    "        # 간단한 gradient 정렬(경계 보존)\n",
    "        def grad(img):\n",
    "            dx = img[:,:,:,1:] - img[:,:,:,:-1]\n",
    "            dy = img[:,:,1:,:] - img[:,:,:-1,:]\n",
    "            dx = F.pad(dx, (0,1,0,0))\n",
    "            dy = F.pad(dy, (0,0,0,1))\n",
    "            return dx, dy\n",
    "        gpmx, gpmy = grad(mu)\n",
    "        gtmx, gtmy = grad(target)\n",
    "        gdiff = (gpmx-gtmx).abs() + (gpmy-gtmy).abs()\n",
    "        gdiff = gdiff * valid\n",
    "\n",
    "        denom = valid.sum().clamp_min(1.0)\n",
    "        return nll.sum()/denom + 0.1 * gdiff.sum()/denom\n",
    "\n",
    "    else:\n",
    "        mu    = pred[:, 0:1]\n",
    "        l1    = (mu - target).abs() * valid\n",
    "\n",
    "        def grad(img):\n",
    "            dx = img[:,:,:,1:] - img[:,:,:,:-1]\n",
    "            dy = img[:,:,1:,:] - img[:,:,:-1,:]\n",
    "            dx = F.pad(dx, (0,1,0,0))\n",
    "            dy = F.pad(dy, (0,0,0,1))\n",
    "            return dx, dy\n",
    "        gpmx, gpmy = grad(mu)\n",
    "        gtmx, gtmy = grad(target)\n",
    "        gdiff = (gpmx-gtmx).abs() + (gpmy-gtmy).abs()\n",
    "        gdiff = gdiff * valid\n",
    "\n",
    "        denom = valid.sum().clamp_min(1.0)\n",
    "        return l1.sum()/denom + 0.5 * gdiff.sum()/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849a3818-39f3-44e5-a430-33d08d7d2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_roi_mask(depth_map, bounding_boxes):\n",
    "    H, W = depth_map.shape[:2]\n",
    "    mask = np.ones((H, W), dtype=np.uint8)\n",
    "\n",
    "    for box in bounding_boxes:\n",
    "        if len(box) < 5:\n",
    "            continue\n",
    "        xmin = max(0, int(box[0]))\n",
    "        ymin = max(0, int(box[1]))\n",
    "        xmax = min(W - 1, int(box[2]))\n",
    "        ymax = min(H - 1, int(box[3]))\n",
    "\n",
    "        mask[ymin:ymax + 1, xmin:xmax + 1] = 0\n",
    "\n",
    "    return mask[..., np.newaxis]\n",
    "\n",
    "def read_xml_file(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for obj in root.findall('.//object'):\n",
    "        name = int(obj.find('name').text)\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(float(bndbox.find('xmin').text))\n",
    "        ymin = int(float(bndbox.find('ymin').text))\n",
    "        xmax = int(float(bndbox.find('xmax').text))\n",
    "        ymax = int(float(bndbox.find('ymax').text))\n",
    "\n",
    "        bounding_boxes.append([xmin, ymin, xmax, ymax, name])\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d9d5bc-b6c3-4605-91af-97d3673b6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 충돌 방지용 상수명\n",
    "E_IDX, W_IDX, S_IDX, N_IDX = 0, 1, 2, 3  # t 채널 순서: [E,W,S,N, Y, R, G, B, M] 가정\n",
    "\n",
    "def _reorder_first4(t: torch.Tensor, new0123):\n",
    "    \"\"\"\n",
    "    t: [C,H,W], C>=4\n",
    "    new0123: 새 [E,W,S,N]가 가리킬 '이전' 인덱스 리스트(예: [S_IDX, N_IDX, W_IDX, E_IDX])\n",
    "    안전하게 dim=0 재배열(index_select) 사용.\n",
    "    \"\"\"\n",
    "    C = int(t.shape[0])\n",
    "    first4 = torch.tensor([int(new0123[0]), int(new0123[1]),\n",
    "                           int(new0123[2]), int(new0123[3])],\n",
    "                           dtype=torch.long, device=t.device)\n",
    "    rest   = torch.arange(4, C, dtype=torch.long, device=t.device)\n",
    "    idx    = torch.cat([first4, rest], dim=0)\n",
    "    return torch.index_select(t, dim=0, index=idx)\n",
    "\n",
    "def oriented_random_transform(t: torch.Tensor, enable_flip=True, enable_rot=True):\n",
    "    \"\"\"\n",
    "    t: [C,H,W], 채널 0..3 = [E,W,S,N]\n",
    "    플립/회전에 따라 EWSN 채널 재배열을 함께 수행.\n",
    "    \"\"\"\n",
    "    # 좌우 플립: E<->W\n",
    "    if enable_flip and random.random() < 0.5:\n",
    "        t = torch.flip(t, dims=[2])                               # W-dim\n",
    "        t = _reorder_first4(t, [W_IDX, E_IDX, S_IDX, N_IDX])\n",
    "\n",
    "    # 상하 플립: S<->N\n",
    "    if enable_flip and random.random() < 0.5:\n",
    "        t = torch.flip(t, dims=[1])                               # H-dim\n",
    "        t = _reorder_first4(t, [E_IDX, W_IDX, N_IDX, S_IDX])\n",
    "\n",
    "    # 90° 회전 k회(반시계)\n",
    "    if enable_rot:\n",
    "        k = random.choice([0,1,2,3])\n",
    "        if k:\n",
    "            t = torch.rot90(t, k=k, dims=(1,2))\n",
    "            if k == 1:        # 90° CCW\n",
    "                t = _reorder_first4(t, [S_IDX, N_IDX, W_IDX, E_IDX])\n",
    "            elif k == 2:      # 180°\n",
    "                t = _reorder_first4(t, [W_IDX, E_IDX, N_IDX, S_IDX])\n",
    "            else:             # 270° CCW (== 90° CW)\n",
    "                t = _reorder_first4(t, [N_IDX, S_IDX, E_IDX, W_IDX])\n",
    "    return t\n",
    "\n",
    "# ---------------- 드롭인 Dataset ----------------\n",
    "class PreloadedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dat_list 항목: numpy array (H, W, 9)\n",
    "      채널 순서: [E, W, S, N, Y(depth), R, G, B, M(mask)]\n",
    "    반환: x_dirs(4), y(1), i_rgb(3), m_mask(1)\n",
    "    - train 모드 + use_aug=True 일 때만 채널-의식 증강 수행\n",
    "    \"\"\"\n",
    "    def __init__(self, dat_list, target_size=None, mode='train', use_aug=True, device='cuda'):\n",
    "        self.data = dat_list\n",
    "        self.target_size = target_size  # (w,h) 사용할 경우 외부에서 리사이즈 처리 권장\n",
    "        self.mode = mode\n",
    "        self.use_aug = use_aug\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = self.data[idx]                               # (H, W, 9)\n",
    "        t = torch.from_numpy(arr).permute(2,0,1).float()   # (9, H, W)\n",
    "\n",
    "        # 채널-의식 증강 (train일 때만)\n",
    "        if self.mode == 'train' and self.use_aug:\n",
    "            t = oriented_random_transform(t, enable_flip=True, enable_rot=True)\n",
    "\n",
    "        # 분리: [E,W,S,N] | depth | RGB | mask\n",
    "        x_dirs = t[0:4]    # (4,H,W)\n",
    "        y      = t[4:5]    # (1,H,W)\n",
    "        i_rgb  = t[5:8]    # (3,H,W)\n",
    "        m_mask = t[8:9]    # (1,H,W)\n",
    "\n",
    "        return (x_dirs.to(self.device),\n",
    "                y.to(self.device),\n",
    "                i_rgb.to(self.device),\n",
    "                m_mask.to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d34659-7394-4dce-a5b6-e9772b236d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca32bb-89ea-49a5-9d3c-95dce991a906",
   "metadata": {},
   "source": [
    "# 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4880257-c7ab-4da9-a005-6f0ba1fd1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [GN] => ReLU) * 2 + SEBlock\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, num_groups=16):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=num_groups, num_channels=mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_groups=16):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels, num_groups=num_groups)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True, num_groups=16):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2, num_groups)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, num_groups=num_groups)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DualInputUNetWithDepthScale(nn.Module):\n",
    "    def __init__(self, bilinear=True, num_groups=16):\n",
    "        super().__init__()\n",
    "        factor = 2 if bilinear else 1\n",
    "\n",
    "        # RGB 경로\n",
    "        self.rgb_inc   = DoubleConv(3, 32, num_groups=num_groups)\n",
    "        # extra 경로: 4ch → 1ch\n",
    "        self.extra_inc = nn.Conv2d(4, 1, kernel_size=1)\n",
    "\n",
    "        # UNet 본체 (RGB feature 32ch → down/upsampling)\n",
    "        self.down1 = Down(32,  64,  num_groups=num_groups)\n",
    "        self.down2 = Down(64, 128,  num_groups=num_groups)\n",
    "        self.down3 = Down(128,256,  num_groups=num_groups)\n",
    "        self.down4 = Down(256,512//factor, num_groups=num_groups)\n",
    "\n",
    "        self.up1 = Up(512,   256//factor, bilinear, num_groups)\n",
    "        self.up2 = Up(256,   128//factor, bilinear, num_groups)\n",
    "        self.up3 = Up(128,    64//factor, bilinear, num_groups)\n",
    "        self.up4 = Up(64,     32,         bilinear, num_groups)\n",
    "\n",
    "        self.outc = OutConv(32, 1)  # normalized depth in [0..1]\n",
    "\n",
    "    def forward(self, rgb, extra):\n",
    "        # 1) RGB 경로로부터 normalized depth map (0..1)\n",
    "        x = self.rgb_inc(rgb)        # (B,32,H,W)\n",
    "        x1 = self.down1(x)           # (B,64,H/2,W/2)\n",
    "        x2 = self.down2(x1)          # ...\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "\n",
    "        x5 = self.up1(x4, x3)\n",
    "        x6 = self.up2(x5, x2)\n",
    "        x7 = self.up3(x6, x1)\n",
    "        x8 = self.up4(x7, x)\n",
    "\n",
    "        depth_norm = torch.sigmoid(self.outc(x8))  # (B,1,H,W) in [0,1]\n",
    "\n",
    "        depth_value = self.extra_inc(extra)      # (B,1,H,W)\n",
    "        depth_value = F.adaptive_avg_pool2d(depth_value, 1)\n",
    "\n",
    "        depth_value = F.relu(depth_value)\n",
    "\n",
    "        # 3) normalized output 에 depth_value 곱해 실제 scale 복원\n",
    "        depth_out = depth_norm * depth_value      # (B,1,H,W)\n",
    "\n",
    "        return depth_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389195b6-b838-4dd3-8499-7707da5a405c",
   "metadata": {},
   "source": [
    "# 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6ea836-b9d4-4670-b438-edfbb89f97de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1205/1205 [01:08<00:00, 17.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2620/2620 [02:09<00:00, 20.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train에는 노이즈없이 깨끗한 데이터 + 라벨링 작업이 완료된 데이터\n",
    "dat_dir   = glob('../데이터/3DImage/train/*/*/*3D_MergeDepth.dat')\n",
    "east_dir  = glob('../데이터/3DImage/train/*/*/3D_Depth_E.dat')\n",
    "west_dir  = glob('../데이터/3DImage/train/*/*/3D_Depth_W.dat')\n",
    "south_dir = glob('../데이터/3DImage/train/*/*/3D_Depth_S.dat')\n",
    "north_dir = glob('../데이터/3DImage/train/*/*/3D_Depth_N.dat')\n",
    "\n",
    "dat_dir = sorted(dat_dir)\n",
    "east_dir = sorted(east_dir)\n",
    "west_dir = sorted(west_dir)\n",
    "south_dir = sorted(south_dir)\n",
    "north_dir = sorted(north_dir)\n",
    "\n",
    "train_set = []\n",
    "\n",
    "for y, e, w, s, n in tqdm(zip(dat_dir, east_dir, west_dir, south_dir, north_dir), total=len(dat_dir)):\n",
    "    Y = load_to_file(y)\n",
    "    E = load_to_file(e)\n",
    "    W = load_to_file(w)\n",
    "    S = load_to_file(s)\n",
    "    N = load_to_file(n)\n",
    "\n",
    "    i = Path(y.replace('3D_MergeDepth.dat', '2D_Vert.bmp'))\n",
    "    I = load_image_file(i)\n",
    "    \n",
    "    m = Path(y.replace('.dat', '.xml'))\n",
    "    if m.exists() :\n",
    "        bbx = read_xml_file(m)\n",
    "        M = create_roi_mask(Y, bbx)\n",
    "    else :\n",
    "        M = np.ones_like(Y)\n",
    "    \n",
    "    data = np.concatenate((E, W, S, N, Y, I, M), axis=-1)\n",
    "    train_set.append(data)\n",
    "\n",
    "# Test에는 라벨링 작업 진행중. 라벨 있는 샘플 -> 학습 및 검증에 사용, 라벨 없는 샘플 -> Test용\n",
    "dat_dir = glob('../데이터/3DImage/test/*/*/*3D_MergeDepth.dat')\n",
    "east_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_E.dat')\n",
    "west_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_W.dat')\n",
    "south_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_S.dat')\n",
    "north_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_N.dat')\n",
    "\n",
    "dat_dir = sorted(dat_dir)\n",
    "east_dir = sorted(east_dir)\n",
    "west_dir = sorted(west_dir)\n",
    "south_dir = sorted(south_dir)\n",
    "north_dir = sorted(north_dir)\n",
    "\n",
    "for y, e, w, s, n in tqdm(zip(dat_dir, east_dir, west_dir, south_dir, north_dir), total=len(dat_dir)):\n",
    "    Y = load_to_file(y)\n",
    "    E = load_to_file(e)\n",
    "    W = load_to_file(w)\n",
    "    S = load_to_file(s)\n",
    "    N = load_to_file(n)\n",
    "\n",
    "    i = Path(y.replace('3D_MergeDepth.dat', '2D_Vert.bmp'))\n",
    "    I = load_image_file(i)\n",
    "    \n",
    "    m = Path(y.replace('.dat', '.xml'))\n",
    "    if m.exists() :\n",
    "        bbx = read_xml_file(m)\n",
    "        M = create_roi_mask(Y, bbx)\n",
    "        data = np.concatenate((E, W, S, N, Y, I, M), axis=-1)\n",
    "        train_set.append(data)\n",
    "    else :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b099afcc-d593-4440-8d60-86f3d15bb32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2620/2620 [01:48<00:00, 24.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = []\n",
    "\n",
    "dat_dir = glob('../데이터/3DImage/test/*/*/*3D_MergeDepth.dat')\n",
    "east_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_E.dat')\n",
    "west_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_W.dat')\n",
    "south_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_S.dat')\n",
    "north_dir = glob('../데이터/3DImage/test/*/*/3D_Depth_N.dat')\n",
    "\n",
    "dat_dir = sorted(dat_dir)\n",
    "east_dir = sorted(east_dir)\n",
    "west_dir = sorted(west_dir)\n",
    "south_dir = sorted(south_dir)\n",
    "north_dir = sorted(north_dir)\n",
    "\n",
    "for y, e, w, s, n in tqdm(zip(dat_dir, east_dir, west_dir, south_dir, north_dir), total=len(dat_dir)):\n",
    "    Y = load_to_file(y)\n",
    "    E = load_to_file(e)\n",
    "    W = load_to_file(w)\n",
    "    S = load_to_file(s)\n",
    "    N = load_to_file(n)\n",
    "\n",
    "    i = Path(y.replace('3D_MergeDepth.dat', '2D_Vert.bmp'))\n",
    "    I = load_image_file(i)\n",
    "    \n",
    "    m = Path(y.replace('.dat', '.xml'))\n",
    "    if not m.exists() :\n",
    "        M = np.ones_like(Y)\n",
    "        data = np.concatenate((E, W, S, N, Y, I, M), axis=-1)\n",
    "        test_set.append(data)\n",
    "    else :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a1f2f5-f819-4854-a498-e3931411d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = train_set\n",
    "\n",
    "val_ratio = 0.15\n",
    "n_total = len(total_dataset)\n",
    "n_val = int(n_total * val_ratio)\n",
    "n_train = n_total - n_val\n",
    "\n",
    "train_set_, valid_set_ = random_split(total_dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91605ec2-175a-4f82-a365-4b1c37587df9",
   "metadata": {},
   "source": [
    "# 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d0d908-341f-4f96-8f5f-cc64567f9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DualInputUNetWithDepthScale(bilinear=True).to(device)\n",
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc738385-2c79-4f41-a065-66c8f885652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6636cd23-e607-4a06-b6d5-c549d61351f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PreloadedDataset(train_set_, mode='train', use_aug=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = PreloadedDataset(valid_set_, mode='valid', use_aug=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b452ac-699e-461b-be41-807121bf5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PreloadedDataset(test_set, mode='valid', use_aug=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd3683-7745-494d-a1a9-226bd0c8f82f",
   "metadata": {},
   "source": [
    "# 학습 시작!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5924a221-0b19-4186-8676-5b5e73718b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_padding(tensor, target_size=(128, 128), mode='bilinear'):\n",
    "    B, C, H, W = tensor.shape\n",
    "    target_h, target_w = target_size\n",
    "\n",
    "    # 원본 크기 저장\n",
    "    original_size = (H, W)\n",
    "\n",
    "    # 종횡비 유지한 리사이즈 크기 계산\n",
    "    scale = min(target_w / W, target_h / H)\n",
    "    new_h, new_w = int(H * scale), int(W * scale)\n",
    "\n",
    "    resized = F.interpolate(\n",
    "        tensor, size=(new_h, new_w), mode=mode,\n",
    "        align_corners=False if mode != 'nearest' else None\n",
    "    )\n",
    "\n",
    "    # 패딩 계산\n",
    "    pad_h = target_h - new_h\n",
    "    pad_w = target_w - new_w\n",
    "\n",
    "    pad_top = pad_h // 2\n",
    "    pad_bottom = pad_h - pad_top\n",
    "    pad_left = pad_w // 2\n",
    "    pad_right = pad_w - pad_left\n",
    "\n",
    "    padded = F.pad(resized, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0)\n",
    "    return padded, original_size\n",
    "\n",
    "# 전체 텐서 리스트 다운샘플 + 크기 정보 저장\n",
    "def downsample_tensor_batch(tensor_list):\n",
    "    x, original_sizes = resize_with_padding(tensor_list[0], mode='bilinear')\n",
    "    y, _ = resize_with_padding(tensor_list[1], mode='bilinear')\n",
    "    i, _ = resize_with_padding(tensor_list[2] / 255., mode='bilinear')\n",
    "    m, _ = resize_with_padding(tensor_list[3], mode='nearest')\n",
    "\n",
    "    return x, y, i, m, original_sizes\n",
    "\n",
    "def restore_to_original(tensor, original_size, mode='nearest'):\n",
    "    return F.interpolate(\n",
    "        tensor, size=original_size,\n",
    "        mode=mode,\n",
    "        align_corners=False if mode != 'nearest' else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa150773-4722-4fc4-9243-c5a2ac93c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path =  r'E:\\project\\3d_noise\\inpainting\\restart\\Moire-Depth\\best_model.pth'\n",
    "# state_dict = torch.load(best_model_path, map_location='cpu', weights_only=True)\n",
    "# model.load_state_dict(state_dict)\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4383ea4b-8e4a-468d-a1c8-872080779603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m x, y, image, mask, _ \u001b[38;5;241m=\u001b[39m downsample_tensor_batch([x, y, image, mask])\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m masked_target \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m mask\n\u001b[0;32m     34\u001b[0m masked_output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m*\u001b[39m mask\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 91\u001b[0m, in \u001b[0;36mDualInputUNetWithDepthScale.forward\u001b[1;34m(self, rgb, extra)\u001b[0m\n\u001b[0;32m     88\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown3(x2)\n\u001b[0;32m     89\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown4(x3)\n\u001b[1;32m---> 91\u001b[0m x5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m x6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x5, x2)\n\u001b[0;32m     93\u001b[0m x7 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x6, x1)\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mUp.forward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     47\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x1, [diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffX \u001b[38;5;241m-\u001b[39m diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     48\u001b[0m                 diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffY \u001b[38;5;241m-\u001b[39m diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x2, x1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda3_env\\envs\\TJKim\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "history = []\n",
    "model.train()\n",
    "best_model = model\n",
    "best_loss = 10000\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "num_epochs = 200000\n",
    "patience = 50\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "confidence_list = []\n",
    "interval_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ###### Train ######\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for x, y, image, mask in train_dataloader:\n",
    "        x, y, image, mask = x.to(device), y.to(device), image.to(device), mask.to(device)\n",
    "        \n",
    "        x, y, image, mask, _ = downsample_tensor_batch([x, y, image, mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image, x)\n",
    "\n",
    "        masked_target = y * mask\n",
    "        masked_output = output * mask\n",
    "        \n",
    "        max_tgt  = masked_target.view(masked_target.shape[0], -1).amax(dim=1).view(-1,1,1,1).clamp_min(1e-6)\n",
    "\n",
    "        loss = custom_loss(masked_output / max_tgt, masked_target / max_tgt)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    ###### Valid ######\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    merged = None  # 저장할 데이터 준비\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x_val, y_val, img_val, mask_val) in enumerate(valid_dataloader):\n",
    "            x_val, y_val, img_val, mask_val = x_val.to(device), y_val.to(device), img_val.to(device), mask_val.to(device)\n",
    "\n",
    "            x_val, y_val, img_val, mask_val, coord = downsample_tensor_batch([x_val, y_val, img_val, mask_val])\n",
    "            \n",
    "            output_val = model(img_val, x_val)\n",
    "\n",
    "            masked_target_val = y_val * mask_val\n",
    "            masked_output_val = output_val * mask_val\n",
    "\n",
    "            max_tgt_val  = masked_target_val.view(masked_target_val.shape[0], -1).amax(dim=1).view(-1,1,1,1).clamp_min(1e-6)\n",
    "            loss_val = custom_loss(masked_target_val / max_tgt_val, masked_output_val / max_tgt_val)\n",
    "            \n",
    "            valid_loss += loss_val.item()\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                gt = y_val[0,0].cpu().numpy()\n",
    "                pred = output_val[0,0].cpu().numpy()\n",
    "                mask_np = mask_val[0,0].cpu().numpy().astype(bool)\n",
    "\n",
    "                rows = np.any(mask_np, axis=1)\n",
    "                cols = np.any(mask_np, axis=0)\n",
    "                gt_crop   = gt[rows][:, cols][..., None]\n",
    "                pred_crop = pred[rows][:, cols][..., None]\n",
    "                merged = np.concatenate([gt_crop, pred_crop], axis=1)\n",
    "\n",
    "    valid_loss_epoch = valid_loss / len(valid_dataloader)\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss_epoch)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss_epoch:.4f} | Counter: {counter}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if valid_loss_epoch < best_loss:\n",
    "        best_loss = valid_loss_epoch\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), '../모델/best_model.pth')\n",
    "        if merged is not None:\n",
    "            save_to_dat_file(f'../모델/output_epoch{epoch+1}.dat', merged)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best val loss: {best_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "    # 실시간 그래프 업데이트\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(valid_loss_history, label=\"Valid Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f87f6-2783-42c5-a4ec-9f5b7142a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정규화 기준, 메디안 3, another_shape 제외, UNet\n",
    "Early stopping at epoch 767. Best val loss: 840.143508\n",
    "\n",
    "## 정규화 기준, 메디안 3, train, another 만 학습, UNet\n",
    "Early stopping at epoch 183. Best val loss: 1401.180986\n",
    "\n",
    "## 정규화 없음, train 학습 UNet(0.5)\n",
    "Early stopping at epoch 199. Valid Loss: 348.4743\n",
    "\n",
    "## 정규화 없음, train, another 학습 UNet(0.5)\n",
    "Early stopping at epoch 594. Valid Loss: 154.7076\n",
    "\n",
    "## 정규화 없음, train, another 학습 UNet(0.5), 배치 1\n",
    "Early stopping at epoch 594. Valid Loss: 154.7076\n",
    "\n",
    "## 정규화 없음, train another 학습 UNet(0.2)\n",
    "Early stopping at epoch 359. Best val loss: 576.785149\n",
    "\n",
    "## 정규화 없음, train another 학습 UNet(0.5)\n",
    "Early stopping at epoch 504. Best val loss: 469.384471\n",
    "\n",
    "## 정규화 없음, train another 학습 UNet(1.0)\n",
    "Early stopping at epoch 217. Best val loss: 1008.170885\n",
    "\n",
    "## zscore 5 -->zcore는 평평한 영역이 뾰족뾰족하게 만들어짐\n",
    "Early stopping at epoch 588. Best val loss: 766.671943\n",
    "\n",
    "## zscore 3\n",
    "Early stopping at epoch 282. Best val loss: 680.499732\n",
    "\n",
    "## zscore 2\n",
    "Early stopping at epoch 255. Best val loss: 843.582516\n",
    "\n",
    "## zscore 1\n",
    "Early stopping at epoch 723. Best val loss: 2313.067925"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a93222-3ecb-4981-8b19-720c0f821c8e",
   "metadata": {},
   "source": [
    "# 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e6e1490-b20f-4040-9fac-8285f3dbd5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = DualInputUNetWithDepthScale(bilinear=True)\n",
    "best_model_path =  r'../모델/best_model.pth'\n",
    "state_dict = torch.load(best_model_path, map_location='cpu', weights_only=True)\n",
    "best_model.load_state_dict(state_dict)\n",
    "best_model = best_model.to('cuda')\n",
    "best_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56cd7a88-3a3a-4131-93a6-676870a58aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] valid_loss(avg): 0.043763, batches: 1448\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "valid_loss_sum = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x_val, y_val, img_val, mask_val) in enumerate(test_dataloader):\n",
    "        x_val   = x_val.to(device)\n",
    "        y_val   = y_val.to(device)\n",
    "        img_val = img_val.to(device)\n",
    "        mask_val= mask_val.to(device)\n",
    "\n",
    "        x_val, y_val, img_val, mask_val, coord = downsample_tensor_batch([x_val, y_val, img_val, mask_val])\n",
    "\n",
    "        # 모델 호출 (그대로)\n",
    "        output_val = model(img_val, x_val)  # [B,1,H,W]\n",
    "\n",
    "        # 손실 (정규화 동일)\n",
    "        masked_target_val = y_val * mask_val\n",
    "        masked_output_val = output_val * mask_val\n",
    "        max_tgt_val = masked_target_val.flatten(1).amax(1).view(-1,1,1,1).clamp_min(1e-6)\n",
    "        loss_val = custom_loss(masked_target_val / max_tgt_val,\n",
    "                               masked_output_val / max_tgt_val)\n",
    "        valid_loss_sum += loss_val.item()\n",
    "        num_batches    += 1\n",
    "\n",
    "        # ===== 저장: 배치 첫 샘플 1장, (h, 2w, 1) 보장 =====\n",
    "        b = 0\n",
    "        gt    = y_val[b,0].detach().cpu().numpy()          # (H,W)\n",
    "        pred  = output_val[b,0].detach().cpu().numpy()     # (H,W)\n",
    "        m_np  = mask_val[b,0].detach().cpu().numpy().astype(bool)\n",
    "\n",
    "        rows = m_np.any(axis=1)\n",
    "        cols = m_np.any(axis=0)\n",
    "        if rows.any() and cols.any():\n",
    "            gt_crop   = gt[np.ix_(rows, cols)]             # (h,w)\n",
    "            pred_crop = pred[np.ix_(rows, cols)]           # (h,w)\n",
    "        else:\n",
    "            gt_crop, pred_crop = gt, pred                  # 마스크 비었으면 원본\n",
    "\n",
    "        # (h,2w,1)로 저장: 채널축 추가 후 가로 concat\n",
    "        merged = np.concatenate([\n",
    "            gt_crop[..., None],         # (h,w,1)\n",
    "            pred_crop[..., None]        # (h,w,1)\n",
    "        ], axis=1).astype(np.float32)   # -> (h,2w,1)\n",
    "\n",
    "        count += 1\n",
    "        save_to_dat_file(f'../데이터/3DImage/output_test/{count:05d}.dat', merged)\n",
    "\n",
    "# 평균 loss\n",
    "valid_loss = valid_loss_sum / max(1, num_batches)\n",
    "print(f\"[TEST] valid_loss(avg): {valid_loss:.6f}, batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a967bf0c-9a37-4da7-9aa0-6e8551726c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SAVE] avg loss: 0.043763, saved samples: 1448\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2  # pip install opencv-python\n",
    "\n",
    "save_root = Path(\"../데이터/3DImage/output_test\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _to_hw1(a2d: np.ndarray) -> np.ndarray:\n",
    "    return a2d[..., None] if a2d.ndim == 2 else a2d\n",
    "\n",
    "def _parse_coord_one(c):\n",
    "    \"\"\"\n",
    "    c: coord for one sample. Supports:\n",
    "      - tuple/list of 4 ints\n",
    "      - 1D tensor of len 4\n",
    "    Returns (y0, x0, y1, x1).\n",
    "    \"\"\"\n",
    "    if hasattr(c, \"detach\"):  # torch tensor\n",
    "        c = c.detach().cpu().tolist()\n",
    "    c = list(map(int, c))\n",
    "    if len(c) != 4:\n",
    "        return None\n",
    "    y0, a, y1, b = c\n",
    "    # try (y0,x0,y1,x1)\n",
    "    x0, x1 = a, b\n",
    "    if y1 > y0 and x1 > x0:\n",
    "        return (y0, x0, y1, x1)\n",
    "    # try (y0,y1,x0,x1)\n",
    "    x0, x1 = y1, b\n",
    "    y1 = a\n",
    "    if y1 > y0 and x1 > x0:\n",
    "        return (y0, x0, y1, x1)\n",
    "    return None\n",
    "\n",
    "count = 0\n",
    "valid_loss_sum = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x_val, y_val, img_val, mask_val) in enumerate(test_dataloader):\n",
    "        # ----- 원본 텐서 보관 (crop용; downsample 전에 복사) -----\n",
    "        x_full   = x_val.to(device)    # [B,4,H0,W0]\n",
    "        y_full   = y_val.to(device)    # [B,1,H0,W0]\n",
    "        img_full = img_val.to(device)  # [B,3,H0,W0]\n",
    "        mask_full= mask_val.to(device) # [B,1,H0,W0]\n",
    "\n",
    "        # ↓ 기존 파이프라인 유지\n",
    "        x_val, y_val, img_val, mask_val, coord = downsample_tensor_batch(\n",
    "            [x_val.to(device), y_val.to(device), img_val.to(device), mask_val.to(device)]\n",
    "        )\n",
    "\n",
    "        # 모델 호출 (그대로)\n",
    "        output_val = model(img_val, x_val)  # [B,1,128,128]\n",
    "\n",
    "        # 손실 (정규화 동일)\n",
    "        masked_target_val = y_val * mask_val\n",
    "        masked_output_val = output_val * mask_val\n",
    "        max_tgt_val = masked_target_val.flatten(1).amax(1).view(-1,1,1,1).clamp_min(1e-6)\n",
    "        loss_val = custom_loss(masked_target_val / max_tgt_val,\n",
    "                               masked_output_val / max_tgt_val)\n",
    "        valid_loss_sum += loss_val.item()\n",
    "        num_batches    += 1\n",
    "\n",
    "        # ===== 저장: 배치 내 모든 샘플 =====\n",
    "        B = output_val.size(0)\n",
    "        for b in range(B):\n",
    "            count += 1\n",
    "            prefix = save_root / f\"sample_{count:05d}\"\n",
    "\n",
    "            # ---- coord 해석 → (y0,x0,y1,x1)\n",
    "            if isinstance(coord, (list, tuple)) and len(coord) == B:\n",
    "                c = coord[b]\n",
    "            else:\n",
    "                c = coord[b] if hasattr(coord, \"shape\") and coord.ndim >= 2 else coord\n",
    "            parsed = _parse_coord_one(c) if c is not None else None\n",
    "\n",
    "            H0, W0 = int(y_full.shape[-2]), int(y_full.shape[-1])\n",
    "            if parsed is not None:\n",
    "                y0, x0, y1, x1 = parsed\n",
    "                y0 = max(0, min(y0, H0)); y1 = max(0, min(y1, H0))\n",
    "                x0 = max(0, min(x0, W0)); x1 = max(0, min(x1, W0))\n",
    "                if not (y1 > y0 and x1 > x0):\n",
    "                    y0, x0, y1, x1 = 0, 0, H0, W0\n",
    "            else:\n",
    "                y0, x0, y1, x1 = 0, 0, H0, W0\n",
    "\n",
    "            # ---- GT crop\n",
    "            gt_crop = y_full[b,0, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)  # (hc,wc)\n",
    "\n",
    "            # ---- Pred 복원 (letterbox 역연산 후 원본 crop 크기로 resize)\n",
    "            pred_128 = output_val[b,0].detach().cpu().numpy().astype(np.float32)  # (128,128)\n",
    "            target = pred_128.shape[0]\n",
    "            hc, wc = gt_crop.shape\n",
    "\n",
    "            scale = min(target / max(wc, 1e-6), target / max(hc, 1e-6))\n",
    "            h_c = int(round(hc * scale))\n",
    "            w_c = int(round(wc * scale))\n",
    "            h_c = max(1, min(h_c, target))\n",
    "            w_c = max(1, min(w_c, target))\n",
    "\n",
    "            pad_y = (target - h_c) // 2\n",
    "            pad_x = (target - w_c) // 2\n",
    "\n",
    "            pred_content = pred_128[pad_y:pad_y+h_c, pad_x:pad_x+w_c]\n",
    "            pred_restored = cv2.resize(pred_content, (wc, hc), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # ---- Vert crop 저장\n",
    "            vert_crop = img_full[b, :, y0:y1, x0:x1].detach().cpu().numpy()  # [3,h,w]\n",
    "            vert_crop = np.transpose(vert_crop, (1,2,0))                     # [h,w,3]\n",
    "            vmax = vert_crop.max()\n",
    "            if vmax <= 1.0:\n",
    "                vert_u8 = np.clip(vert_crop * 255.0, 0, 255).astype(np.uint8)\n",
    "            else:\n",
    "                vert_u8 = np.clip(vert_crop, 0, 255).astype(np.uint8)\n",
    "            cv2.imwrite(str(prefix.with_name(prefix.stem + \"_vert.png\")), vert_u8[:, :, ::-1])\n",
    "\n",
    "            # ---- E/W/S/N crop 2×2 저장\n",
    "            E_crop = x_full[b,0, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "            W_crop = x_full[b,1, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "            S_crop = x_full[b,2, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "            N_crop = x_full[b,3, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "            top_row    = np.concatenate([E_crop, W_crop], axis=1)\n",
    "            bottom_row = np.concatenate([S_crop, N_crop], axis=1)\n",
    "            ewsn_2x2   = np.concatenate([top_row, bottom_row], axis=0)\n",
    "            save_to_dat_file(str(prefix.with_name(prefix.stem + \"_EWSN_2x2.dat\")), _to_hw1(ewsn_2x2))\n",
    "\n",
    "            # ---- GT | Pred crop 좌우 합치기\n",
    "            gt_pred_lr = np.concatenate([gt_crop, pred_restored], axis=1)   # (hc, 2*wc)\n",
    "            save_to_dat_file(str(prefix.with_name(prefix.stem + \"_GT_PRED_crop.dat\")),\n",
    "                             _to_hw1(gt_pred_lr))\n",
    "\n",
    "# 평균 loss 출력\n",
    "valid_loss = valid_loss_sum / max(1, num_batches)\n",
    "print(f\"[TEST SAVE] avg loss: {valid_loss:.6f}, saved samples: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f9d2eed-b710-41a1-9773-eb012d4b4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_dat_file(file_name, data):\n",
    "    \"\"\"\n",
    "    data: (H,W,1) 또는 (H,W,3) 또는 (H,W)\n",
    "      - (H,W,3)인 경우 axis=2 평균 (원래 코드와 동일 동작)\n",
    "      - (H,W,1)은 채널 제거\n",
    "      - NaN/Inf → 0, 음수 → 0\n",
    "      - 0 ~ max(data) 범위로 클리핑한 뒤 float32 리틀엔디언으로 저장\n",
    "    파일 포맷:\n",
    "      [4바이트 width][4바이트 height][height*width개의 float32 little-endian]\n",
    "    \"\"\"\n",
    "    arr = np.array(data)\n",
    "\n",
    "    # 채널 차원 처리 (원 코드와 호환)\n",
    "    if arr.ndim == 3:\n",
    "        if arr.shape[2] == 1:\n",
    "            arr = arr[:, :, 0]\n",
    "        else:\n",
    "            # (H,W,3) → 평균\n",
    "            arr = np.mean(arr, axis=2)\n",
    "    elif arr.ndim > 3:\n",
    "        # 혹시 이상 차원이면 squeeze 후 다시 처리\n",
    "        arr = np.squeeze(arr)\n",
    "        if arr.ndim == 3:\n",
    "            if arr.shape[2] == 1:\n",
    "                arr = arr[:, :, 0]\n",
    "            else:\n",
    "                arr = np.mean(arr, axis=2)\n",
    "\n",
    "    # 안전화: NaN/Inf 제거, 음수 제거\n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=False)\n",
    "    np.maximum(arr, 0.0, out=arr)  # 음수 → 0\n",
    "\n",
    "    # vmax 계산 및 0~vmax 클리핑\n",
    "    vmax = float(arr.max()) if arr.size else 0.0\n",
    "    if not np.isfinite(vmax) or vmax <= 0.0:\n",
    "        vmax = 1e-6\n",
    "    np.clip(arr, 0.0, vmax, out=arr)\n",
    "\n",
    "    height, width = arr.shape\n",
    "\n",
    "    # little-endian float32로 직렬화하여 저장 (루프보다 빠름)\n",
    "    with open(file_name, 'wb') as fout:\n",
    "        fout.write(width.to_bytes(4, byteorder='little', signed=False))\n",
    "        fout.write(height.to_bytes(4, byteorder='little', signed=False))\n",
    "        fout.write(arr.astype('<f4', copy=False).tobytes(order='C'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1a8d639-8c83-440b-8cc4-d69e808b2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def make_vert_on_pred_u8(\n",
    "    vert_crop: np.ndarray,\n",
    "    pred_restored: np.ndarray,\n",
    "    mask_crop: np.ndarray | None = None,\n",
    "    alpha: float = 0.45,\n",
    "    colormap: int = cv2.COLORMAP_JET,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    vert_crop : (H,W,3) RGB, float[0..1] 또는 [0..255]\n",
    "    pred_restored : (H,W) float, 값 범위는 자유\n",
    "    mask_crop : (H,W) float/bool, 1=유효/0=무시 (optional)\n",
    "    alpha : heatmap 가중치 (0~1)\n",
    "    colormap : OpenCV 컬러맵 (기본 JET)\n",
    "\n",
    "    return : (H,W,3) RGB uint8 (imwrite 전에 BGR 변환 필요)\n",
    "    \"\"\"\n",
    "    assert vert_crop.ndim == 3 and vert_crop.shape[2] == 3, \"vert_crop must be (H,W,3)\"\n",
    "    assert pred_restored.ndim == 2, \"pred_restored must be (H,W)\"\n",
    "    H, W, _ = vert_crop.shape\n",
    "    assert pred_restored.shape == (H, W), \"shape mismatch between vert_crop and pred_restored\"\n",
    "\n",
    "    # --- 1) 입력 클린업 ---\n",
    "    vert = vert_crop.astype(np.float32)\n",
    "    vmax = float(np.nanmax(vert))\n",
    "    if vmax <= 1.0 + 1e-6:\n",
    "        vert = np.clip(vert * 255.0, 0, 255)\n",
    "    else:\n",
    "        vert = np.clip(vert, 0, 255)\n",
    "\n",
    "    pred = pred_restored.astype(np.float32)\n",
    "    if mask_crop is not None:\n",
    "        m = (mask_crop > 0.5).astype(np.float32)\n",
    "        # 마스크 밖은 NaN으로 두고 min-max 구해 안정화\n",
    "        valid = np.isfinite(pred) & (m > 0)\n",
    "        if np.any(valid):\n",
    "            p = pred.copy()\n",
    "            p[~valid] = np.nan\n",
    "            p_min = np.nanmin(p)\n",
    "            p_max = np.nanmax(p)\n",
    "        else:\n",
    "            # 유효값이 없으면 전부 0으로\n",
    "            p_min, p_max = 0.0, 1.0\n",
    "    else:\n",
    "        m = None\n",
    "        valid = np.isfinite(pred)\n",
    "        if np.any(valid):\n",
    "            p_min = float(np.nanmin(pred))\n",
    "            p_max = float(np.nanmax(pred))\n",
    "        else:\n",
    "            p_min, p_max = 0.0, 1.0\n",
    "\n",
    "    # --- 2) pred 정규화 [0,1] → [0,255] ---\n",
    "    if p_max - p_min < 1e-8:\n",
    "        pred_norm = np.zeros_like(pred, dtype=np.float32)\n",
    "    else:\n",
    "        pred_norm = (pred - p_min) / (p_max - p_min)\n",
    "        pred_norm = np.clip(pred_norm, 0.0, 1.0)\n",
    "\n",
    "    pred_u8 = (pred_norm * 255.0).astype(np.uint8)\n",
    "\n",
    "    # --- 3) 컬러맵 적용 (BGR) 후 RGB로 변환 ---\n",
    "    heat_bgr = cv2.applyColorMap(pred_u8, colormap)          # (H,W,3) BGR uint8\n",
    "    heat = cv2.cvtColor(heat_bgr, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "\n",
    "    # --- 4) 알파 블렌딩 (마스크가 있으면 유효 영역에서만) ---\n",
    "    alpha = float(np.clip(alpha, 0.0, 1.0))\n",
    "    if m is None:\n",
    "        out = (1.0 - alpha) * vert + alpha * heat\n",
    "    else:\n",
    "        m3 = m[:, :, None]  # (H,W,1)\n",
    "        out = vert.copy()\n",
    "        out = (1.0 - alpha * m3) * out + (alpha * m3) * heat\n",
    "\n",
    "    out = np.clip(out, 0, 255).astype(np.uint8)  # RGB uint8\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "419646a9-fd53-42fb-8a55-448b4296a5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST SAVE] avg loss: 0.043763, saved samples: 1448\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2  # pip install opencv-python\n",
    "import torch\n",
    "\n",
    "# ==== 사용자 설정 ====\n",
    "save_root = Path(\"../데이터/3DImage/output_test\")\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "CENTER_PAD = True   # keep_aspect letterbox가 '센터 패딩'이면 True, '좌상단 패딩'이면 False\n",
    "\n",
    "# ==== 헬퍼 ====\n",
    "def _to_hw1(a2d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"(H,W)->(H,W,1) 보장\"\"\"\n",
    "    return a2d[..., None] if a2d.ndim == 2 else a2d\n",
    "\n",
    "def _parse_coord_one(c):\n",
    "    \"\"\"\n",
    "    c: one sample coord; supports list/tuple/1D tensor of 4 numbers.\n",
    "    Tries (y0,x0,y1,x1) then (y0,y1,x0,x1). Returns (y0,x0,y1,x1) or None.\n",
    "    \"\"\"\n",
    "    if hasattr(c, \"detach\"):\n",
    "        c = c.detach().cpu().tolist()\n",
    "    c = list(map(int, c))\n",
    "    if len(c) != 4: \n",
    "        return None\n",
    "    y0, a, y1, b = c\n",
    "    # try (y0,x0,y1,x1)\n",
    "    x0, x1 = a, b\n",
    "    if y1 > y0 and x1 > x0:\n",
    "        return (y0, x0, y1, x1)\n",
    "    # try (y0,y1,x0,x1)\n",
    "    x0, x1 = y1, b\n",
    "    y1 = a\n",
    "    if y1 > y0 and x1 > x0:\n",
    "        return (y0, x0, y1, x1)\n",
    "    return None\n",
    "\n",
    "def _restore_pred_to_crop(pred_128: np.ndarray, hc: int, wc: int, center_pad: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    128x128 pred (letterbox된 네트워크 출력) → 원본 crop 크기(hc,wc)로 복원\n",
    "    keep_aspect_ratio=True 가정. center_pad=False면 좌상단 패딩 가정.\n",
    "    \"\"\"\n",
    "    target = pred_128.shape[0]  # 128\n",
    "    # 원본 크기(hc,wc) → letterbox 유효크기(h_c,w_c)\n",
    "    scale = min(target / max(wc, 1e-6), target / max(hc, 1e-6))\n",
    "    h_c = int(round(hc * scale))\n",
    "    w_c = int(round(wc * scale))\n",
    "    h_c = max(1, min(h_c, target))\n",
    "    w_c = max(1, min(w_c, target))\n",
    "\n",
    "    if center_pad:\n",
    "        pad_y = (target - h_c) // 2\n",
    "        pad_x = (target - w_c) // 2\n",
    "    else:\n",
    "        pad_y, pad_x = 0, 0\n",
    "\n",
    "    y0_p, y1_p = pad_y, pad_y + h_c\n",
    "    x0_p, x1_p = pad_x, pad_x + w_c\n",
    "    # clamp\n",
    "    y0_p = max(0, min(y0_p, target)); y1_p = max(0, min(y1_p, target))\n",
    "    x0_p = max(0, min(x0_p, target)); x1_p = max(0, min(x1_p, target))\n",
    "\n",
    "    pred_content = pred_128[y0_p:y1_p, x0_p:x1_p]                # (h_c, w_c)\n",
    "    pred_restored = cv2.resize(pred_content, (wc, hc), interpolation=cv2.INTER_CUBIC)  # (hc,wc)\n",
    "    return pred_restored.astype(np.float32)\n",
    "\n",
    "# ==== 저장 루프 (모델/파이프라인 그대로) ====\n",
    "count = 0\n",
    "valid_loss_sum = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x_val, y_val, img_val, mask_val) in enumerate(test_dataloader):\n",
    "        # 원본 텐서(크롭용) 보관\n",
    "        x_full   = x_val.to(device)    # [B,4,H0,W0]\n",
    "        y_full   = y_val.to(device)    # [B,1,H0,W0]\n",
    "        img_full = img_val.to(device)  # [B,3,H0,W0]\n",
    "        mask_full= mask_val.to(device) # [B,1,H0,W0]\n",
    "\n",
    "        # 기존 파이프라인 유지 (downsample + coord)\n",
    "        x_val, y_val, img_val, mask_val, coord = downsample_tensor_batch(\n",
    "            [x_val.to(device), y_val.to(device), img_val.to(device), mask_val.to(device)]\n",
    "        )\n",
    "\n",
    "        # forward (그대로)\n",
    "        output_val = model(img_val, x_val)  # [B,1,128,128] 가정\n",
    "\n",
    "        # loss (그대로)\n",
    "        masked_target_val = y_val * mask_val\n",
    "        masked_output_val = output_val * mask_val\n",
    "        max_tgt_val = masked_target_val.flatten(1).amax(1).view(-1,1,1,1).clamp_min(1e-6)\n",
    "        loss_val = custom_loss(masked_target_val / max_tgt_val,\n",
    "                               masked_output_val / max_tgt_val)\n",
    "        valid_loss_sum += loss_val.item()\n",
    "        num_batches    += 1\n",
    "\n",
    "        # === 저장: 배치 내 모든 샘플 ===\n",
    "        B = output_val.size(0)\n",
    "        for b in range(B):\n",
    "            count += 1\n",
    "            prefix = save_root / f\"sample_{count:05d}\"\n",
    "\n",
    "            # coord → (y0,x0,y1,x1)\n",
    "            if isinstance(coord, (list, tuple)) and len(coord) == B:\n",
    "                c = coord[b]\n",
    "            else:\n",
    "                c = coord[b] if hasattr(coord, \"shape\") and coord.ndim >= 2 else coord\n",
    "            parsed = _parse_coord_one(c) if c is not None else None\n",
    "\n",
    "            H0, W0 = int(y_full.shape[-2]), int(y_full.shape[-1])\n",
    "            if parsed is not None:\n",
    "                y0, x0, y1, x1 = parsed\n",
    "                y0 = max(0, min(y0, H0)); y1 = max(0, min(y1, H0))\n",
    "                x0 = max(0, min(x0, W0)); x1 = max(0, min(x1, W0))\n",
    "                if not (y1 > y0 and x1 > x0):\n",
    "                    y0, x0, y1, x1 = 0, 0, H0, W0\n",
    "            else:\n",
    "                y0, x0, y1, x1 = 0, 0, H0, W0\n",
    "\n",
    "            # 원본 crop 추출\n",
    "            gt_crop   = y_full[b,0, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)  # (hc,wc)\n",
    "            E_crop    = x_full[b,0, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "            W_crop    = x_full[b,1, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "            S_crop    = x_full[b,2, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "            N_crop    = x_full[b,3, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "            vert_crop = img_full[b, :, y0:y1, x0:x1].detach().cpu().numpy()                  # [3,hc,wc]\n",
    "            vert_crop = np.transpose(vert_crop, (1,2,0))                                     # (hc,wc,3)\n",
    "            mask_crop = mask_full[b,0, y0:y1, x0:x1].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "            hc, wc = gt_crop.shape\n",
    "\n",
    "            # pred 복원 (letterbox 역연산 → 원본 crop 크기로)\n",
    "            pred_128 = output_val[b,0].detach().cpu().numpy().astype(np.float32)  # (128,128)\n",
    "            pred_restored = _restore_pred_to_crop(pred_128, hc, wc, center_pad=CENTER_PAD)\n",
    "\n",
    "            # --- (A) GT|Pred 좌우 결합 저장 (원본 crop 기준, 동일 크기) ---\n",
    "            gt_pred_lr = np.concatenate([gt_crop, pred_restored], axis=1)   # (hc, 2*wc)\n",
    "            save_to_dat_file(str(prefix.with_name(prefix.stem + \"_GT_PRED_crop.dat\")),\n",
    "                             _to_hw1(gt_pred_lr).astype(np.float32))\n",
    "\n",
    "            # --- (B) E/W/S/N 2×2 저장 (원본 crop) ---\n",
    "            top_row    = np.concatenate([E_crop, W_crop], axis=1)           # (hc, 2*wc)\n",
    "            bottom_row = np.concatenate([S_crop, N_crop], axis=1)           # (hc, 2*wc)\n",
    "            ewsn_2x2   = np.concatenate([top_row, bottom_row], axis=0)      # (2*hc, 2*wc)\n",
    "            save_to_dat_file(str(prefix.with_name(prefix.stem + \"_EWSN_2x2_crop.dat\")),\n",
    "                             _to_hw1(ewsn_2x2).astype(np.float32))\n",
    "\n",
    "            # --- (C) Vert crop PNG 저장 ---\n",
    "            vmax = vert_crop.max()\n",
    "            if vmax <= 1.0:\n",
    "                vert_u8 = np.clip(vert_crop * 255.0, 0, 255).astype(np.uint8)\n",
    "            else:\n",
    "                vert_u8 = np.clip(vert_crop, 0, 255).astype(np.uint8)\n",
    "            cv2.imwrite(str(prefix.with_name(prefix.stem + \"_vert_crop.png\")), vert_u8[:, :, ::-1])  # RGB->BGR\n",
    "\n",
    "            # --- (D) Mask_map PNG 저장 (0/1 → 0/255) ---\n",
    "            mask_u8 = ((1-mask_crop) * 255.0).clip(0,255).astype(np.uint8)\n",
    "            cv2.imwrite(str(prefix.with_name(prefix.stem + \"_mask.png\")), mask_u8)\n",
    "\n",
    "            vert_on_pred_u8 = make_vert_on_pred_u8(vert_crop, pred_restored, mask_crop, alpha=0.45)\n",
    "            cv2.imwrite(str(prefix.with_name(prefix.stem + \"_vert_on_pred.png\")), vert_on_pred_u8[:, :, ::-1])  # RGB->BGR\n",
    "\n",
    "# 평균 loss\n",
    "valid_loss = valid_loss_sum / max(1, num_batches)\n",
    "print(f\"[TEST SAVE] avg loss: {valid_loss:.6f}, saved samples: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2c55b-572c-40b1-97aa-c56ffba60580",
   "metadata": {},
   "outputs": [],
   "source": [
    "[TRAIN] avg loss: 0.013649\n",
    "[VALID] avg loss: 0.023416\n",
    "[TEST]  avg loss: 0.042289"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8c21d-daeb-4372-b627-04488b0cd672",
   "metadata": {},
   "source": [
    "# 추론 시간 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25a551e0-411b-4efd-8440-f935cf946e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cpu_prep+cpu_inf #10] prep   0.52±0.08 ms | H2D   0.00±0.00 ms | INF  12.84±1.06 ms | D2H   0.00±0.00 ms\n",
      "[cpu_prep+cpu_inf #20] prep   0.50±0.07 ms | H2D   0.00±0.00 ms | INF  12.32±0.98 ms | D2H   0.00±0.00 ms\n",
      "[cpu_prep+cpu_inf #30] prep   0.50±0.07 ms | H2D   0.00±0.00 ms | INF  12.43±0.97 ms | D2H   0.00±0.00 ms\n",
      "[cpu_prep+cpu_inf #40] prep   0.52±0.08 ms | H2D   0.00±0.00 ms | INF  12.45±0.93 ms | D2H   0.00±0.00 ms\n",
      "[cpu_prep+cpu_inf #50] prep   0.53±0.08 ms | H2D   0.00±0.00 ms | INF  12.45±0.88 ms | D2H   0.00±0.00 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SGLEE\\AppData\\Local\\Temp\\ipykernel_16412\\1298197007.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  autocast_ctx = torch.cuda.amp.autocast(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cpu_prep+gpu_inf #10] prep   0.52±0.11 ms | H2D   0.29±0.03 ms | INF   8.80±0.73 ms | D2H   0.09±0.01 ms\n",
      "[cpu_prep+gpu_inf #20] prep   0.54±0.11 ms | H2D   0.27±0.17 ms | INF   6.20±3.26 ms | D2H   0.07±0.02 ms\n",
      "[cpu_prep+gpu_inf #30] prep   0.53±0.10 ms | H2D   0.22±0.15 ms | INF   4.94±3.20 ms | D2H   0.06±0.02 ms\n",
      "[cpu_prep+gpu_inf #40] prep   0.54±0.09 ms | H2D   0.20±0.14 ms | INF   4.33±2.96 ms | D2H   0.06±0.02 ms\n",
      "[cpu_prep+gpu_inf #50] prep   0.54±0.08 ms | H2D   0.19±0.13 ms | INF   3.99±2.73 ms | D2H   0.06±0.02 ms\n",
      "[gpu_prep+cpu_inf #10] prep   0.27±0.02 ms | H2D   0.24±0.05 ms | INF  13.01±0.61 ms | D2H   0.13±0.02 ms\n",
      "[gpu_prep+cpu_inf #20] prep   0.27±0.03 ms | H2D   0.25±0.07 ms | INF  13.04±0.56 ms | D2H   0.13±0.02 ms\n",
      "[gpu_prep+cpu_inf #30] prep   0.27±0.02 ms | H2D   0.24±0.06 ms | INF  12.90±0.75 ms | D2H   0.12±0.02 ms\n",
      "[gpu_prep+cpu_inf #40] prep   0.27±0.02 ms | H2D   0.23±0.06 ms | INF  12.87±0.76 ms | D2H   0.12±0.02 ms\n",
      "[gpu_prep+cpu_inf #50] prep   0.27±0.02 ms | H2D   0.23±0.05 ms | INF  12.88±0.78 ms | D2H   0.12±0.02 ms\n",
      "[gpu_prep+gpu_inf #10] prep   0.25±0.09 ms | H2D   0.13±0.04 ms | INF   2.18±0.20 ms | D2H   0.04±0.00 ms\n",
      "[gpu_prep+gpu_inf #20] prep   0.24±0.07 ms | H2D   0.14±0.05 ms | INF   2.21±0.24 ms | D2H   0.04±0.00 ms\n",
      "[gpu_prep+gpu_inf #30] prep   0.25±0.06 ms | H2D   0.14±0.04 ms | INF   2.33±0.25 ms | D2H   0.04±0.00 ms\n",
      "[gpu_prep+gpu_inf #40] prep   0.25±0.06 ms | H2D   0.14±0.04 ms | INF   2.40±0.28 ms | D2H   0.04±0.00 ms\n",
      "[gpu_prep+gpu_inf #50] prep   0.25±0.05 ms | H2D   0.14±0.03 ms | INF   2.46±0.33 ms | D2H   0.04±0.00 ms\n",
      "\n",
      "==== Timing Summary (mean ± std, ms) ====\n",
      "PIPELINE                     PREP          H2D          INF          D2H        TOTAL\n",
      "cpu_prep+cpu_inf        0.53± 0.08    0.00± 0.00   12.45± 0.88    0.00± 0.00   12.98\n",
      "cpu_prep+gpu_inf        0.54± 0.08    0.19± 0.13    3.99± 2.73    0.06± 0.02    4.77\n",
      "gpu_prep+cpu_inf        0.27± 0.02    0.23± 0.05   12.88± 0.78    0.12± 0.02   13.50\n",
      "gpu_prep+gpu_inf        0.25± 0.05    0.14± 0.03    2.46± 0.33    0.04± 0.00    2.89\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# =========================================\n",
    "# 공용 유틸\n",
    "# =========================================\n",
    "def _now(): return time.perf_counter()\n",
    "\n",
    "def _mean_std(xs):\n",
    "    xs = list(xs)\n",
    "    if not xs: return 0.0, 0.0\n",
    "    m = sum(xs)/len(xs)\n",
    "    v = sum((x-m)**2 for x in xs) / max(1, len(xs)-1)\n",
    "    return m, math.sqrt(v)\n",
    "\n",
    "def _ensure_cpu(obj):\n",
    "    \"\"\"배치 첫 단계: 무조건 CPU로 시작시킨다.\"\"\"\n",
    "    if torch.is_tensor(obj): return obj.detach().cpu()\n",
    "    if isinstance(obj, (list, tuple)): return type(obj)(_ensure_cpu(x) for x in obj)\n",
    "    if isinstance(obj, dict): return {k:_ensure_cpu(v) for k,v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "def _to(obj, device, non_blocking=True):\n",
    "    \"\"\"컨테이너 안 텐서까지 지정 device로 이동.\"\"\"\n",
    "    if torch.is_tensor(obj): return obj.to(device, non_blocking=non_blocking)\n",
    "    if isinstance(obj, (list, tuple)): return type(obj)(_to(x, device, non_blocking) for x in obj)\n",
    "    if isinstance(obj, dict): return {k:_to(v, device, non_blocking) for k,v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "def _sync_if_cuda(device):\n",
    "    if isinstance(device, str):\n",
    "        is_cuda = device.startswith(\"cuda\")\n",
    "    else:\n",
    "        is_cuda = (device.type == \"cuda\")\n",
    "    if is_cuda and torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "# =========================================\n",
    "# 4 경로 벤치마크\n",
    "# =========================================\n",
    "def run_pipeline_1_cpu_prep_cpu_inf(model, dataloader, *, warmup=5, max_batches=50, print_every=10):\n",
    "    \"\"\"\n",
    "    1) CPU 전처리 -> CPU 추론\n",
    "    시작: CPU / 종료: CPU\n",
    "    \"\"\"\n",
    "    device_model = torch.device(\"cpu\")\n",
    "    model_cpu = model.to(device_model).eval()\n",
    "\n",
    "    t_prep=[]; t_h2d=[]; t_inf=[]; t_d2h=[]\n",
    "    seen=0; warm=0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for it, batch in enumerate(dataloader):\n",
    "            # 항상 CPU에서 시작\n",
    "            batch = _ensure_cpu(batch)\n",
    "            # 배치 언팩\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) >= 4:\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[:4]\n",
    "            elif isinstance(batch, dict):\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[\"x\"], batch[\"y\"], batch[\"img\"], batch[\"mask\"]\n",
    "            else:\n",
    "                raise RuntimeError(\"dataloader는 (x,y,img,mask)를 반환해야 합니다.\")\n",
    "\n",
    "            # (1) CPU 전처리\n",
    "            t0 = _now()\n",
    "            x_p, y_p, img_p, mask_p, *rest = downsample_tensor_batch([x_cpu, y_cpu, img_cpu, mask_cpu])\n",
    "            t1 = _now(); prep_t = t1 - t0\n",
    "\n",
    "            # (2) H2D 없음 (CPU->CPU)\n",
    "            h2d_t = 0.0\n",
    "\n",
    "            # (3) CPU 추론\n",
    "            t2 = _now()\n",
    "            out_cpu = model_cpu(img_p, x_p)  # CPU에서 수행\n",
    "            t3 = _now(); inf_t = t3 - t2\n",
    "\n",
    "            # (4) D2H 없음 (이미 CPU)\n",
    "            d2h_t = 0.0\n",
    "\n",
    "            if warm < warmup:\n",
    "                warm += 1\n",
    "            else:\n",
    "                t_prep.append(prep_t); t_h2d.append(h2d_t); t_inf.append(inf_t); t_d2h.append(d2h_t)\n",
    "                seen += 1\n",
    "                if seen % print_every == 0:\n",
    "                    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "                    print(f\"[cpu_prep+cpu_inf #{seen}] prep {mp*1e3:6.2f}±{sp*1e3:4.2f} ms | \"\n",
    "                          f\"H2D {mh*1e3:6.2f}±{sh*1e3:4.2f} ms | \"\n",
    "                          f\"INF {mi*1e3:6.2f}±{si*1e3:4.2f} ms | \"\n",
    "                          f\"D2H {md*1e3:6.2f}±{sd*1e3:4.2f} ms\")\n",
    "                if max_batches and seen >= max_batches: break\n",
    "\n",
    "    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "    total = mp+mh+mi+md\n",
    "    return {\"name\":\"cpu_prep+cpu_inf\",\"prep\":mp,\"prep_std\":sp,\"h2d\":mh,\"h2d_std\":sh,\"inf\":mi,\"inf_std\":si,\"d2h\":md,\"d2h_std\":sd,\"total\":total,\"measured\":len(t_inf),\"warmup\":warm}\n",
    "\n",
    "def run_pipeline_2_cpu_prep_gpu_inf(model, dataloader, *, warmup=5, max_batches=50, print_every=10, use_amp=True):\n",
    "    \"\"\"\n",
    "    2) CPU 전처리 -> GPU 추론\n",
    "    시작: CPU / 종료: CPU\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"CUDA 필요\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_gpu = model.to(device).eval()\n",
    "\n",
    "    t_prep=[]; t_h2d=[]; t_inf=[]; t_d2h=[]\n",
    "    seen=0; warm=0\n",
    "    autocast_ctx = torch.cuda.amp.autocast(enabled=use_amp)\n",
    "\n",
    "    with torch.inference_mode(), autocast_ctx:\n",
    "        for it, batch in enumerate(dataloader):\n",
    "            batch = _ensure_cpu(batch)\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) >= 4:\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[:4]\n",
    "            elif isinstance(batch, dict):\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[\"x\"], batch[\"y\"], batch[\"img\"], batch[\"mask\"]\n",
    "            else:\n",
    "                raise RuntimeError(\"dataloader는 (x,y,img,mask)를 반환해야 합니다.\")\n",
    "\n",
    "            # (1) CPU 전처리\n",
    "            t0 = _now()\n",
    "            x_p, y_p, img_p, mask_p, *rest = downsample_tensor_batch([x_cpu, y_cpu, img_cpu, mask_cpu])\n",
    "            t1 = _now(); prep_t = t1 - t0\n",
    "\n",
    "            # (2) H2D (필요 입력만 업로드: img, x)\n",
    "            t2 = _now()\n",
    "            x_dev   = _to(x_p, device, non_blocking=True)\n",
    "            img_dev = _to(img_p, device, non_blocking=True)\n",
    "            _sync_if_cuda(device)\n",
    "            t3 = _now(); h2d_t = t3 - t2\n",
    "\n",
    "            # (3) GPU 추론\n",
    "            t4 = _now()\n",
    "            out_dev = model_gpu(img_dev, x_dev)\n",
    "            _sync_if_cuda(device)\n",
    "            t5 = _now(); inf_t = t5 - t4\n",
    "\n",
    "            # (4) D2H (결과 CPU로)\n",
    "            t6 = _now()\n",
    "            out_cpu = out_dev.detach().cpu()\n",
    "            t7 = _now(); d2h_t = t7 - t6\n",
    "\n",
    "            if warm < warmup:\n",
    "                warm += 1\n",
    "            else:\n",
    "                t_prep.append(prep_t); t_h2d.append(h2d_t); t_inf.append(inf_t); t_d2h.append(d2h_t)\n",
    "                seen += 1\n",
    "                if seen % print_every == 0:\n",
    "                    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "                    print(f\"[cpu_prep+gpu_inf #{seen}] prep {mp*1e3:6.2f}±{sp*1e3:4.2f} ms | \"\n",
    "                          f\"H2D {mh*1e3:6.2f}±{sh*1e3:4.2f} ms | \"\n",
    "                          f\"INF {mi*1e3:6.2f}±{si*1e3:4.2f} ms | \"\n",
    "                          f\"D2H {md*1e3:6.2f}±{sd*1e3:4.2f} ms\")\n",
    "                if max_batches and seen >= max_batches: break\n",
    "\n",
    "    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "    total = mp+mh+mi+md\n",
    "    return {\"name\":\"cpu_prep+gpu_inf\",\"prep\":mp,\"prep_std\":sp,\"h2d\":mh,\"h2d_std\":sh,\"inf\":mi,\"inf_std\":si,\"d2h\":md,\"d2h_std\":sd,\"total\":total,\"measured\":len(t_inf),\"warmup\":warm}\n",
    "\n",
    "def run_pipeline_3_gpu_prep_cpu_inf(model, dataloader, *, warmup=5, max_batches=50, print_every=10):\n",
    "    \"\"\"\n",
    "    3) GPU 전처리 -> CPU 추론\n",
    "    시작: CPU / 종료: CPU\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"CUDA 필요\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_cpu = model.to(\"cpu\").eval()  # 추론은 CPU에서!\n",
    "\n",
    "    t_prep=[]; t_h2d=[]; t_inf=[]; t_d2h=[]\n",
    "    seen=0; warm=0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for it, batch in enumerate(dataloader):\n",
    "            batch = _ensure_cpu(batch)\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) >= 4:\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[:4]\n",
    "            elif isinstance(batch, dict):\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[\"x\"], batch[\"y\"], batch[\"img\"], batch[\"mask\"]\n",
    "            else:\n",
    "                raise RuntimeError(\"dataloader는 (x,y,img,mask)를 반환해야 합니다.\")\n",
    "\n",
    "            # (1) H2D (원본을 GPU로)\n",
    "            t2 = _now()\n",
    "            x_dev   = _to(x_cpu, device, non_blocking=True)\n",
    "            y_dev   = _to(y_cpu, device, non_blocking=True)\n",
    "            img_dev = _to(img_cpu, device, non_blocking=True)\n",
    "            mask_dev= _to(mask_cpu, device, non_blocking=True)\n",
    "            _sync_if_cuda(device)\n",
    "            t3 = _now(); h2d_t = t3 - t2\n",
    "\n",
    "            # (2) GPU 전처리\n",
    "            t0 = _now()\n",
    "            x_p, y_p, img_p, mask_p, *rest = downsample_tensor_batch([x_dev, y_dev, img_dev, mask_dev])\n",
    "            _sync_if_cuda(device)\n",
    "            t1 = _now(); prep_t = t1 - t0\n",
    "\n",
    "            # (3) D2H (전처리 결과를 CPU로 내려서 CPU 추론에 사용)\n",
    "            t6 = _now()\n",
    "            x_cpu_p   = x_p.detach().cpu()\n",
    "            img_cpu_p = img_p.detach().cpu()\n",
    "            t7 = _now(); d2h_t = t7 - t6\n",
    "\n",
    "            # (4) CPU 추론\n",
    "            t4 = _now()\n",
    "            out_cpu = model_cpu(img_cpu_p, x_cpu_p)\n",
    "            t5 = _now(); inf_t = t5 - t4\n",
    "\n",
    "            if warm < warmup:\n",
    "                warm += 1\n",
    "            else:\n",
    "                t_prep.append(prep_t); t_h2d.append(h2d_t); t_inf.append(inf_t); t_d2h.append(d2h_t)\n",
    "                seen += 1\n",
    "                if seen % print_every == 0:\n",
    "                    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "                    print(f\"[gpu_prep+cpu_inf #{seen}] prep {mp*1e3:6.2f}±{sp*1e3:4.2f} ms | \"\n",
    "                          f\"H2D {mh*1e3:6.2f}±{sh*1e3:4.2f} ms | \"\n",
    "                          f\"INF {mi*1e3:6.2f}±{si*1e3:4.2f} ms | \"\n",
    "                          f\"D2H {md*1e3:6.2f}±{sd*1e3:4.2f} ms\")\n",
    "                if max_batches and seen >= max_batches: break\n",
    "\n",
    "    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "    total = mp+mh+mi+md\n",
    "    return {\"name\":\"gpu_prep+cpu_inf\",\"prep\":mp,\"prep_std\":sp,\"h2d\":mh,\"h2d_std\":sh,\"inf\":mi,\"inf_std\":si,\"d2h\":md,\"d2h_std\":sd,\"total\":total,\"measured\":len(t_inf),\"warmup\":warm}\n",
    "\n",
    "def run_pipeline_4_gpu_prep_gpu_inf(model, dataloader, *, warmup=5, max_batches=50, print_every=10, use_amp=True):\n",
    "    \"\"\"\n",
    "    4) GPU 전처리 -> GPU 추론\n",
    "    시작: CPU / 종료: CPU\n",
    "    - y/mask는 전처리에 필요 shape만 맞추는 더미 텐서를 GPU에서 생성하여 전달\n",
    "    \"\"\"\n",
    "    assert torch.cuda.is_available(), \"CUDA 필요\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    model_gpu = model.to(device).eval()\n",
    "\n",
    "    t_prep=[]; t_h2d=[]; t_inf=[]; t_d2h=[]\n",
    "    seen=0; warm=0\n",
    "\n",
    "    # 권장: 최신 API로 변경 (FutureWarning 제거)\n",
    "    autocast_ctx = torch.amp.autocast(device_type=\"cuda\", enabled=use_amp)\n",
    "\n",
    "    with torch.inference_mode(), autocast_ctx:\n",
    "        for it, batch in enumerate(dataloader):\n",
    "            # 0) 항상 CPU에서 시작\n",
    "            batch = _ensure_cpu(batch)\n",
    "            if isinstance(batch, (list, tuple)) and len(batch) >= 4:\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[:4]\n",
    "            elif isinstance(batch, dict):\n",
    "                x_cpu, y_cpu, img_cpu, mask_cpu = batch[\"x\"], batch[\"y\"], batch[\"img\"], batch[\"mask\"]\n",
    "            else:\n",
    "                raise RuntimeError(\"dataloader는 (x,y,img,mask)를 반환해야 합니다.\")\n",
    "\n",
    "            # 1) H2D: 추론에 필요한 최소 텐서 업로드 (x, img)\n",
    "            t2 = _now()\n",
    "            x_dev   = _to(x_cpu, device, non_blocking=True)   # [B,4,H,W]\n",
    "            img_dev = _to(img_cpu, device, non_blocking=True) # [B,3,H,W]\n",
    "            _sync_if_cuda(device)\n",
    "            t3 = _now(); h2d_t = t3 - t2\n",
    "\n",
    "            # 2) GPU 전처리:\n",
    "            #    y/mask가 None이면 downsample가 shape 참조 시 에러 → 더미 생성\n",
    "            #    더미는 앞단 shape만 맞추면 됨. dtype은 x_dev와 일치시킴.\n",
    "            B, _, H, W = x_dev.shape\n",
    "            y_dev_dummy   = torch.zeros((B, 1, H, W), device=device, dtype=x_dev.dtype)\n",
    "            mask_dev_dummy= torch.ones((B, 1, H, W),  device=device, dtype=x_dev.dtype)\n",
    "\n",
    "            t0 = _now()\n",
    "            x_p, _, img_p, _, *rest = downsample_tensor_batch([x_dev, y_dev_dummy, img_dev, mask_dev_dummy])\n",
    "            _sync_if_cuda(device)\n",
    "            t1 = _now(); prep_t = t1 - t0\n",
    "\n",
    "            # (선택) channels_last 적용 (이미지 경로 가속)\n",
    "            if img_p.is_cuda:\n",
    "                img_p = img_p.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "            # 3) GPU 추론\n",
    "            t4 = _now()\n",
    "            out_dev = model_gpu(img_p, x_p)\n",
    "            _sync_if_cuda(device)\n",
    "            t5 = _now(); inf_t = t5 - t4\n",
    "\n",
    "            # 4) D2H: 결과 CPU로 내려서 종료를 CPU로 보장\n",
    "            t6 = _now()\n",
    "            out_cpu = out_dev.detach().cpu()\n",
    "            t7 = _now(); d2h_t = t7 - t6\n",
    "\n",
    "            if warm < warmup:\n",
    "                warm += 1\n",
    "            else:\n",
    "                t_prep.append(prep_t); t_h2d.append(h2d_t); t_inf.append(inf_t); t_d2h.append(d2h_t)\n",
    "                seen += 1\n",
    "                if seen % print_every == 0:\n",
    "                    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "                    print(f\"[gpu_prep+gpu_inf #{seen}] prep {mp*1e3:6.2f}±{sp*1e3:4.2f} ms | \"\n",
    "                          f\"H2D {mh*1e3:6.2f}±{sh*1e3:4.2f} ms | \"\n",
    "                          f\"INF {mi*1e3:6.2f}±{si*1e3:4.2f} ms | \"\n",
    "                          f\"D2H {md*1e3:6.2f}±{sd*1e3:4.2f} ms\")\n",
    "                if max_batches and seen >= max_batches: break\n",
    "\n",
    "    mp,sp=_mean_std(t_prep); mh,sh=_mean_std(t_h2d); mi,si=_mean_std(t_inf); md,sd=_mean_std(t_d2h)\n",
    "    total = mp+mh+mi+md\n",
    "    return {\"name\":\"gpu_prep+gpu_inf\",\"prep\":mp,\"prep_std\":sp,\"h2d\":mh,\"h2d_std\":sh,\"inf\":mi,\"inf_std\":si,\"d2h\":md,\"d2h_std\":sd,\"total\":total,\"measured\":len(t_inf),\"warmup\":warm}\n",
    "\n",
    "# =========================================\n",
    "# 리포트\n",
    "# =========================================\n",
    "def print_summary(*stats_list):\n",
    "    def ms(x): return x*1e3\n",
    "    print(\"\\n==== Timing Summary (mean ± std, ms) ====\")\n",
    "    print(f\"{'PIPELINE':20s} {'PREP':>12s} {'H2D':>12s} {'INF':>12s} {'D2H':>12s} {'TOTAL':>12s}\")\n",
    "    for st in stats_list:\n",
    "        print(f\"{st['name']:20s} \"\n",
    "              f\"{ms(st['prep']):7.2f}±{ms(st['prep_std']):5.2f} \"\n",
    "              f\"{ms(st['h2d']):7.2f}±{ms(st['h2d_std']):5.2f} \"\n",
    "              f\"{ms(st['inf']):7.2f}±{ms(st['inf_std']):5.2f} \"\n",
    "              f\"{ms(st['d2h']):7.2f}±{ms(st['d2h_std']):5.2f} \"\n",
    "              f\"{ms(st['total']):7.2f}\")\n",
    "\n",
    "# =========================================\n",
    "# 사용 예시\n",
    "# =========================================\n",
    "# DataLoader 예:\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "#                              num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "# 모델/전처리 함수는 기존 것 사용 (downsample_tensor_batch, model)\n",
    "\n",
    "# AMP/Autotune(고정 해상도면 권장)\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# 실행:\n",
    "s1 = run_pipeline_1_cpu_prep_cpu_inf(model, test_dataloader, warmup=5, max_batches=50)\n",
    "s2 = run_pipeline_2_cpu_prep_gpu_inf(model, test_dataloader, warmup=5, max_batches=50, use_amp=True)\n",
    "s3 = run_pipeline_3_gpu_prep_cpu_inf(model, test_dataloader, warmup=5, max_batches=50)\n",
    "s4 = run_pipeline_4_gpu_prep_gpu_inf(model, test_dataloader, warmup=5, max_batches=50, use_amp=True)\n",
    "print_summary(s1, s2, s3, s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087413f4-dfa9-42ff-a634-c341e18eeb61",
   "metadata": {},
   "source": [
    "# 메모리 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a933e98-5c4a-4f05-b869-17f118176cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 총 메모리 사용량]\n",
      "현재 할당(allocated): 73.78 MB\n",
      "예약(reserved)     : 100.00 MB\n",
      "피크(peak)         : 81.78 MB\n"
     ]
    }
   ],
   "source": [
    "def get_total_gpu_memory(model, dataloader, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    x, y, img, mask = next(iter(dataloader))\n",
    "    img = img.to(device)\n",
    "    x   = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(img, x)\n",
    "\n",
    "    allocated = torch.cuda.memory_allocated(device) / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved(device) / 1024**2\n",
    "    peak = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "\n",
    "    print(f\"[GPU 총 메모리 사용량]\")\n",
    "    print(f\"현재 할당(allocated): {allocated:.2f} MB\")\n",
    "    print(f\"예약(reserved)     : {reserved:.2f} MB\")\n",
    "    print(f\"피크(peak)         : {peak:.2f} MB\")\n",
    "\n",
    "get_total_gpu_memory(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36043cc-dc28-40a1-96c4-a1e606853b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a97570-94b5-4afb-ae9e-536d8df19fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
